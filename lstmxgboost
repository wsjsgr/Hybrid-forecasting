import numpy as np
import pandas as pd
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
from bayes_opt import BayesianOptimization
from sklearn.metrics import mean_squared_error
import xgboost as xgb

def lstm_modeling(dataframe, i):

    scaler = MinMaxScaler

    scaled_data = scaler.fit_transform(dataframe)
    
    X = scaled_data[:, :-1]
    y = scaled_data[:, -1]
    
    #X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    train_size = int(len(X) * 0.8)
    
    X_train, X_test = X[0:train_size, :], X[train_size:len(X), :]
    y_train, y_test = y[0:train_size], y[train_size:len(y)]
      
    X_train = X_train.reshape(X_train.shape[0], 1, X_train.shape[1])
    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])
    
    def create_model(lr, neurons, dropout_rate):
        model = Sequential()
        model.add(LSTM(neurons, activation='relu', return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))
        model.add(Dropout(dropout_rate))
        model.add(LSTM(neurons, activation='relu'))
        model.add(Dropout(dropout_rate))
        model.add(Dense(1))
        model.compile(optimizer='adam', loss='mse', metrics=['mae'])
        return model

    def train_model(lr, neurons, dropout_rate):
        model = create_model(lr, int(neurons), dropout_rate)
        history = model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test), verbose=0, shuffle=False)
        val_mae = history.history['val_mae'][-1]
        return -val_mae

    pbounds = {
        'lr': (0.0001, 0.01),
        'neurons': (30, 100),
        'dropout_rate': (0.1, 0.5) 
    }
    
    optimizer = BayesianOptimization(
        f=train_model,
        pbounds=pbounds,
        random_state=1,
    )
    
    optimizer.maximize(init_points=20, n_iter=30)  

    best_params = optimizer.max['params']
    model = create_model(best_params['lr'], int(best_params['neurons']), best_params['dropout_rate'])
    #model = create_model(best_params['lr'], int(best_params['neurons']), best_params['dropout_rate'],best_params['reg_lambda'])
    model.fit(X_train, y_train, epochs=100, batch_size=32, validation_data=(X_test, y_test), verbose=1, shuffle=False)
    
    y_pred = model.predict(X_test)
    y_pred_original = scaler.inverse_transform(np.hstack((X_test.reshape(X_test.shape[0], X_test.shape[2]), y_pred)))

    y_train_pred = model.predict(X_train)
    y_train_pred_original = scaler.inverse_transform(np.hstack((X_train.reshape(X_train.shape[0], X_train.shape[2]), y_train_pred)))
        
    y_train_pred = pd.DataFrame()
    y_test_pred = pd.DataFrame()

    y_train_pred = y_train_pred_original[:, -1]
    y_test_pred = y_pred_original[:, -1]
    
    return y_train_pred, y_test_pred

def xgboost_modeling(dataframe, i):
    scaler = MinMaxScaler()  
    scaled_data = scaler.fit_transform(dataframe)
    
    X = scaled_data[:, :-1]
    y = scaled_data[:, -1]

    train_size = int(len(X) * 0.8)
    #test_size = len(X) - train_size

    X_train, X_test = X[0:train_size, :], X[train_size:len(X), :]
    y_train, y_test = y[0:train_size], y[train_size:len(y)]


    def create_model(learning_rate, n_estimators, max_depth, subsample, colsample_bytree,lasso,ridge):
        model = xgb.XGBRegressor(learning_rate=learning_rate, 
                             n_estimators=int(n_estimators),
                             max_depth=int(max_depth),subsample=subsample,
                             colsample_bytree=colsample_bytree,
                             reg_alpha=lasso, 
                             reg_lambda=ridge,  
                             objective='reg:squarederror')
        return model

    def train_model(learning_rate, n_estimators, max_depth, subsample, colsample_bytree,lasso,ridge):
        model = create_model(learning_rate, n_estimators, max_depth, subsample, colsample_bytree,lasso,ridge)
        model.fit(X_train, y_train)
        y_pred = model.predict(X_test)
        mae = mean_squared_error(y_test, y_pred)
        return -mae

    pbounds = {
        'learning_rate': (0.01, 0.08),#(0.01, 0.3)
        'n_estimators': (10, 50),#(50, 500)
        'max_depth': (1, 3),#(3, 10)
        'subsample': (0.7, 1.0),
        'colsample_bytree': (0.7, 1.0),
        'lasso': (0, 5), 
        'ridge': (0, 5)
    }

    optimizer = BayesianOptimization(
        f=train_model,
        pbounds=pbounds,
        random_state=1,
        allow_duplicate_points=True
    )

    optimizer.maximize(init_points=50, n_iter=60)

    best_params = optimizer.max['params']
    model = create_model(best_params['learning_rate'], 
                         best_params['n_estimators'],
                         best_params['max_depth'], 
                         best_params['subsample'], 
                         best_params['colsample_bytree'],
                         best_params['lasso'],
                         best_params['ridge'])

    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    y_pred_original = scaler.inverse_transform(np.hstack((X_test, y_pred.reshape(-1, 1))))

    y_train_pred = model.predict(X_train)
    y_train_pred_original = scaler.inverse_transform(np.hstack((X_train, y_train_pred.reshape(-1, 1))))
    
    y_train_pred = pd.DataFrame()
    y_test_pred = pd.DataFrame()

    y_train_pred = y_train_pred_original[:, -1]
    y_test_pred = y_pred_original[:, -1]
    
    return y_train_pred, y_test_pred
